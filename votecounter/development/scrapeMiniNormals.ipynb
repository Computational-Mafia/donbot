{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Archived Mini Normals from Mafiascum.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy Structure/Lingo:\n",
    "**Spiders** extract data **items**, which Scrapy send one by one to a configured **item pipeline** (if there is possible) to do post-processing on the items.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant packages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.item import Item, Field\n",
    "from scrapy.selector import Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perpage = 25\n",
    "\n",
    "class PostItem(scrapy.Item):\n",
    "    pagelink = scrapy.Field()\n",
    "    forum = scrapy.Field()\n",
    "    thread = scrapy.Field()\n",
    "    number = scrapy.Field()\n",
    "    timestamp = scrapy.Field()\n",
    "    user = scrapy.Field()\n",
    "    content = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define what happens to scrape output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following pipeline stores all scraped items (from all spiders) \n",
    "# into a single items.jl file, containing one item per line serialized \n",
    "# in JSON format:\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # operations performed when spider starts\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('posts.jl', 'w')\n",
    "\n",
    "    # when the spider finishes\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # when the spider yields an item\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define spider..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MafiaScumSpider(scrapy.Spider):\n",
    "    name = 'mafiascum'\n",
    "    \n",
    "    # define set of threads we're going to scrape from (ie all of them)\n",
    "    start_urls = [each[:each.find('\\n')] for each in open('archive.txt').read().split('\\n\\n\\n')]\n",
    "        \n",
    "    # settings\n",
    "    custom_settings = {'LOG_LEVEL': logging.WARNING,\n",
    "                      'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}}\n",
    "\n",
    "    # get page counts and then do the REAL parse on every single page\n",
    "    def parse(self, response):\n",
    "        # find page count \n",
    "        try:\n",
    "            postcount = Selector(response).xpath(\n",
    "                '//div[@class=\"pagination\"]/text()').extract()\n",
    "            postcount = int(postcount[0][4:postcount[0].find(' ')])\n",
    "\n",
    "            # yield parse for every page of thread\n",
    "            for i in range(math.ceil(postcount/perpage)):\n",
    "                yield scrapy.Request(response.url+'&start='+str(i*perpage),\n",
    "                                    callback=self.parse_page)\n",
    "        except IndexError: # if can't, the thread probably doesn't exist\n",
    "            return\n",
    "        \n",
    "        \n",
    "    def parse_page(self, response):\n",
    "        # scan through posts on page and yield Post items for each\n",
    "        sel = Selector(response)\n",
    "        location = sel.xpath('//div[@id=\"page-body\"]/h2/a/@href').extract()[0]\n",
    "        forum = location[location.find('f=')+2:location.find('&t=')]\n",
    "        if location.count('&') == 1:\n",
    "            thread = location[location.find('&t=')+3:]\n",
    "        elif location.count('&') == 2:\n",
    "            thread = location[\n",
    "                location.find('&t=')+3:location.rfind('&')]\n",
    "        \n",
    "        posts = (sel.xpath('//div[@class=\"post bg1\"]') +\n",
    "                 sel.xpath('//div[@class=\"post bg2\"]'))\n",
    "        \n",
    "        for p in posts:\n",
    "            post = PostItem()\n",
    "            post['forum'] = forum\n",
    "            post['thread'] = thread\n",
    "            post['pagelink'] = response.url\n",
    "            try:\n",
    "                post['number'] = p.xpath(\n",
    "                    'div/div[@class=\"postbody\"]/p/a[2]/strong/text()').extract()[0][1:]\n",
    "            except IndexError:\n",
    "                post['number'] = p.xpath(\n",
    "                    'div[@class=\"postbody\"]/p/a[2]/strong/text()').extract()[0][1:]\n",
    "            \n",
    "            try:\n",
    "                post['timestamp'] = p.xpath(\n",
    "                    'div/div/p/text()[4]').extract()[0][23:-4]\n",
    "            except IndexError:\n",
    "                post['timestamp'] = p.xpath(\n",
    "                    'div[@class=\"postbody\"]/p/text()[4]').extract()[0][23:-4]\n",
    "            \n",
    "            try:\n",
    "                post['user'] = p.xpath('div/div/dl/dt/a/text()').extract()[0]\n",
    "            except IndexError:\n",
    "                post['user'] = '<<DELETED_USER>>'\n",
    "                \n",
    "            try:\n",
    "                post['content'] = p.xpath(\n",
    "                    'div/div/div[@class=\"content\"]').extract()[0][21:-6]\n",
    "            except IndexError:\n",
    "                post['content'] = p.xpath(\n",
    "                    'div[@class=\"postbody\"]/div[@class=\"content\"]').extract()[0][21:-6]\n",
    "            \n",
    "            yield post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start scraping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-25 10:26:08 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n",
      "2017-09-25 10:26:08 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(MafiaScumSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and output should be a json file in same directory as this notebook! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leftover Code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open mini normal archive\n",
    "\n",
    "# ??? i don't remember what this does; probably helped me collect archive links some time ago\n",
    "runthis = False\n",
    "\n",
    "if runthis:\n",
    "    # relevant packages\n",
    "    from selenium import webdriver\n",
    "    from scrapy.selector import Selector\n",
    "    import re\n",
    "\n",
    "    # configure browser\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'\n",
    "    options.add_argument('window-size=800x841')\n",
    "    driver = webdriver.Chrome(chrome_options=options)\n",
    "\n",
    "    # get the thread titles and links\n",
    "    links = []\n",
    "    titles = []\n",
    "    for i in range(0, 400, 100):\n",
    "        driver.get('https://forum.mafiascum.net/viewforum.php?f=53&start=' + str(i))\n",
    "        sel = Selector(text=driver.page_source)\n",
    "        links += sel.xpath('//div[@class=\"forumbg\"]/div/ul[@class=\"topiclist topics\"]/li/dl/dt/a[1]/@href').extract()\n",
    "        titles += sel.xpath('//div[@class=\"forumbg\"]/div/ul[@class=\"topiclist topics\"]/li/dl/dt/a[1]/text()').extract()\n",
    "\n",
    "    # formatting, excluding needless threads...\n",
    "    titles = titles[1:]\n",
    "    links = links[1:]\n",
    "    del links[titles.index('Mini Normal Archives')]\n",
    "    del titles[titles.index('Mini Normal Archives')]\n",
    "    titles = [re.search(r'\\d+', each).group(0) for each in titles]\n",
    "\n",
    "    # match txt archive game numbers with forum archive game numbers to find links\n",
    "    f = open('archive.txt', 'r')\n",
    "    txtarchives = f.read().split('\\n\\n\\n')\n",
    "    numbers = [re.search(r'\\d+', each[:each.find('\\n')]).group(0) for each in txtarchives]\n",
    "    f.close()\n",
    "\n",
    "    # store the result...\n",
    "    for i, n in enumerate(numbers):\n",
    "        txtarchives[i] = 'http://forum.mafiascum.net' + links[titles.index(n)][1:] + '\\n' + txtarchives[i]\n",
    "    f = open('archive2.txt', 'w')\n",
    "    f.write('\\n\\n\\n'.join(txtarchives))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
