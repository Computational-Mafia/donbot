{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Thread Scraping\n",
        "\n",
        "Here we provide an example of how to scrape threads using donbot and the `scrapy` library. Scrapy can scrape threads much more efficiently than donbot, because it's designed to make multiple requests in parallel, and it's also designed to be able to scrape multiple pages of a website. All this means that it can scrape threads much faster than donbot can.\n",
        "\n",
        "This example demonstrates the interoperability of donbot with other libraries and its usefulness for basic research activities. In this case, we use scrapy to manage requests across multiple threads and store asynchronously collected posts data, and donbot to parse the HTML and extract the posts."
      ],
      "metadata": {
        "id": "4q7tF73P9Cqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F5q7UCYV89pX"
      },
      "outputs": [],
      "source": [
        "# @title ## Provide a thread url . Press the play button to the left to generate!\n",
        "thread = \"https://forum.mafiascum.net/viewtopic.php?t=12551\" # @param {type:\"string\"}\n",
        "urls = [thread]\n",
        "\n",
        "try:\n",
        "    import scrapy\n",
        "except ImportError:\n",
        "    !pip install -q scrapy\n",
        "    import scrapy\n",
        "import math\n",
        "import logging\n",
        "import json\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from lxml import html\n",
        "try:\n",
        "    from donbot.operations import count_posts, get_posts\n",
        "except ImportError:\n",
        "    !pip install -q donbot-python\n",
        "    from donbot.operations import count_posts, get_posts\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "posts_per_page = 25\n",
        "\n",
        "\n",
        "class PostItem(scrapy.Item):\n",
        "    number = scrapy.Field()\n",
        "    id = scrapy.Field()\n",
        "    user = scrapy.Field()\n",
        "    content = scrapy.Field()\n",
        "    time = scrapy.Field()\n",
        "    pagelink = scrapy.Field()\n",
        "    forum = scrapy.Field()\n",
        "    thread = scrapy.Field()\n",
        "\n",
        "\n",
        "# The following pipeline stores all scraped items (from all spiders)\n",
        "# into a single jsonl file, containing one item per line serialized\n",
        "# in JSON format:\n",
        "class JsonWriterPipeline(object):\n",
        "    # operations performed when spider starts\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open(\"posts.jsonl\", \"w\")\n",
        "\n",
        "    # when the spider finishes\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    # when the spider yields an item\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item\n",
        "\n",
        "\n",
        "class MafiaScumSpider(scrapy.Spider):\n",
        "    name = \"mafiascum\"\n",
        "\n",
        "    # settings\n",
        "    custom_settings = {\n",
        "        \"LOG_LEVEL\": logging.WARNING,\n",
        "        \"ITEM_PIPELINES\": {\"__main__.JsonWriterPipeline\": 1},\n",
        "    }\n",
        "\n",
        "    def start_requests(self):\n",
        "        \"Generates scrapy.Request objects for each URL in the 'archive.txt' file.\"\n",
        "\n",
        "        for url in tqdm(urls):\n",
        "            yield scrapy.Request(url=url, callback=self.request_each_page)\n",
        "\n",
        "    def request_each_page(self, response):\n",
        "        \"Generates scrapy.Request objects for each page of a thread.\"\n",
        "        try:\n",
        "            thread = response.url\n",
        "            post_count = count_posts(html.fromstring(response.body))\n",
        "            end_page_id = math.floor(post_count / posts_per_page) * posts_per_page\n",
        "\n",
        "            for page_id in range(0, end_page_id, posts_per_page):\n",
        "                yield scrapy.Request(\n",
        "                    f\"{thread}&start={str(page_id)}\",\n",
        "                    callback=self.process_posts,\n",
        "                )\n",
        "        except IndexError:\n",
        "            return  # occurs when the requested thread doesn't exist or is empty (?)\n",
        "\n",
        "    def process_posts(self, response):\n",
        "        \"Extracts post data from a page of a thread.\"\n",
        "        thread_page_html = html.fromstring(response.body)\n",
        "        posts = get_posts(thread_page_html)\n",
        "        page_link = response.url\n",
        "        thread = page_link[page_link.find(\"&t=\") + 3 : page_link.find(\"&start\")]\n",
        "        forum = page_link[page_link.find(\"f=\") + 2 : page_link.find(\"&t=\")]\n",
        "        for post in posts:\n",
        "            yield PostItem(\n",
        "                {\"pagelink\": page_link, \"forum\": forum, \"thread\": thread, **post}\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Start scraping...\n",
        "    process = CrawlerProcess()\n",
        "    process.crawl(MafiaScumSpider)\n",
        "    process.start()\n",
        "    files.download('posts.jsonl')\n",
        "\n",
        "\n"
      ]
    }
  ]
}