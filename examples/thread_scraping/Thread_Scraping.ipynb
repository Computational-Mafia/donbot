{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q7tF73P9Cqc"
      },
      "source": [
        "# Thread Scraping\n",
        "\n",
        "Here we provide an example of how to scrape threads using donbot and the `scrapy` library. Scrapy can scrape threads much more efficiently than donbot, because it's designed to make multiple requests in parallel, and it's also designed to be able to scrape multiple pages of a website. All this means that it can scrape threads much faster than donbot can.\n",
        "\n",
        "This example demonstrates the interoperability of donbot with other libraries and its usefulness for basic research activities. In this case, we use scrapy to manage requests across multiple threads and store asynchronously collected posts data, and donbot to parse the HTML and extract the posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F5q7UCYV89pX"
      },
      "outputs": [],
      "source": [
        "# @title ## Provide a thread url . Press the play button to the left to generate!\n",
        "thread = \"https://forum.mafiascum.net/viewtopic.php?t=12551\" # @param {type:\"string\"}\n",
        "urls = [thread]\n",
        "\n",
        "try:\n",
        "    import scrapy\n",
        "except ImportError:\n",
        "    !pip install -q scrapy\n",
        "    import scrapy\n",
        "import math\n",
        "from math import floor\n",
        "import logging\n",
        "import json\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from lxml import html\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from google.colab import files\n",
        "from lxml import html\n",
        "from lxml.html import HtmlElement\n",
        "\n",
        "posts_per_page = 25\n",
        "\n",
        "\n",
        "def count_posts(thread_html: HtmlElement) -> int:\n",
        "    \"\"\"\n",
        "    Counts the number of posts in the specified thread.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    thread_html : HtmlElement\n",
        "        The HTML of a page from the thread to count posts in.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        The number of posts in the specified thread.\n",
        "    \"\"\"\n",
        "    post_count_path = \"//div[@class='pagination']/text()\"\n",
        "    post_count_element = next(\n",
        "        el for el in thread_html.xpath(post_count_path) if el.strip()\n",
        "    )\n",
        "    return int(\"\".join([c for c in post_count_element if c.isdigit()]))\n",
        "\n",
        "\n",
        "def get_thread_page_urls(\n",
        "    thread: str, thread_page_html: HtmlElement, start: int = 0, end: int = -1\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Get the URLs of the pages of a thread.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    thread : str\n",
        "        The URL of the thread.\n",
        "    thread_page_html : HtmlElement\n",
        "        The HTML of a page from the thread.\n",
        "    end : int\n",
        "        The number of pages to retrieve.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[str]\n",
        "        The URLs of the pages of the thread.\n",
        "    \"\"\"\n",
        "    end = end if end != -1 else count_posts(thread_page_html)\n",
        "\n",
        "    posts_per_page = 25\n",
        "    start_page_id = floor(start / posts_per_page) * posts_per_page\n",
        "    end_page_id = floor(end / posts_per_page) * posts_per_page\n",
        "\n",
        "    return [\n",
        "        f\"{thread}&start={str(page_id)}\"\n",
        "        for page_id in range(start_page_id, end_page_id + 1, posts_per_page)\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_post(post_html: HtmlElement) -> dict:  # sourcery skip: merge-dict-assign\n",
        "    \"\"\"\n",
        "    Extracts the data of a post from the post HTML.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    post_html : HtmlElement\n",
        "        The HTML of a post.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        The post's data, including post `id`, `number`, `user, `time`, and `content`.\n",
        "    \"\"\"\n",
        "    post_number_path = \".//span[@class='post-number-bolded']//text()\"\n",
        "    post_user_path = \".//a[@class='username' or @class='username-coloured']/text()\"\n",
        "    post_user_id_path = \".//a[@class='username' or @class='username-coloured']/@href\"\n",
        "    post_content_path = \".//div[@class='content']\"\n",
        "    post_timestamp_path = \".//p[@class='author modified']/text()\"\n",
        "    post_id_path = \".//a/@href\"\n",
        "\n",
        "    post = {}\n",
        "    post[\"number\"] = int(post_html.xpath(post_number_path)[0][1:])\n",
        "    post[\"id\"] = post_html.xpath(post_id_path)[0]\n",
        "    post[\"id\"] = post[\"id\"][post[\"id\"].rfind(\"#\") + 2 :]\n",
        "    post[\"user\"] = post_html.xpath(post_user_path)[0]\n",
        "    post[\"user_id\"] = post_html.xpath(post_user_id_path)[0]\n",
        "    post[\"user_id\"] = post[\"user_id\"][post[\"user_id\"].rfind(\"=\") + 1 :]\n",
        "    post[\"content\"] = html.tostring(post_html.xpath(post_content_path)[0])\n",
        "    post[\"content\"] = post[\"content\"].decode(\"UTF-8\").strip()[21:-6]\n",
        "    post[\"time\"] = post_html.xpath(post_timestamp_path)[-1]\n",
        "    post[\"time\"] = post[\"time\"][post[\"time\"].find(\"Â» \") + 2 :].strip()\n",
        "    return post\n",
        "\n",
        "\n",
        "def get_posts(\n",
        "    thread_page_html: HtmlElement, start: int = 0, end: int | float = -1\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Retrieve posts from a thread.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    thread_page_html : HtmlElement\n",
        "        The HTML of a page from the thread to retrieve posts from.\n",
        "    start : int\n",
        "        Lowest post number to retrieve.\n",
        "    end : int, optional\n",
        "        Highest post number to retrieve.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[dict]\n",
        "        Each post's data, including post `id`, `number`, `user, `time`, and `content`.\n",
        "    \"\"\"\n",
        "    posts = []\n",
        "    end = end if end != -1 else float(\"inf\")\n",
        "    for raw_post in thread_page_html.xpath(\"//div[@class='postbody']\"):\n",
        "        post = get_post(raw_post)\n",
        "        if post[\"number\"] >= start and post[\"number\"] <= end:\n",
        "            posts.append(post)\n",
        "    return posts\n",
        "\n",
        "\n",
        "class PostItem(scrapy.Item):\n",
        "    number = scrapy.Field()\n",
        "    id = scrapy.Field()\n",
        "    user = scrapy.Field()\n",
        "    content = scrapy.Field()\n",
        "    time = scrapy.Field()\n",
        "    pagelink = scrapy.Field()\n",
        "    forum = scrapy.Field()\n",
        "    thread = scrapy.Field()\n",
        "\n",
        "\n",
        "# The following pipeline stores all scraped items (from all spiders)\n",
        "# into a single jsonl file, containing one item per line serialized\n",
        "# in JSON format:\n",
        "class JsonWriterPipeline(object):\n",
        "    # operations performed when spider starts\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open(\"posts.jsonl\", \"w\")\n",
        "\n",
        "    # when the spider finishes\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    # when the spider yields an item\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item\n",
        "\n",
        "\n",
        "class MafiaScumSpider(scrapy.Spider):\n",
        "    name = \"mafiascum\"\n",
        "\n",
        "    # settings\n",
        "    custom_settings = {\n",
        "        \"LOG_LEVEL\": logging.WARNING,\n",
        "        \"ITEM_PIPELINES\": {\"__main__.JsonWriterPipeline\": 1},\n",
        "    }\n",
        "\n",
        "    def start_requests(self):\n",
        "        \"Generates scrapy.Request objects for each URL in the 'archive.txt' file.\"\n",
        "\n",
        "        for url in tqdm(urls):\n",
        "            yield scrapy.Request(url=url, callback=self.request_each_page)\n",
        "\n",
        "    def request_each_page(self, response):\n",
        "        \"Generates scrapy.Request objects for each page of a thread.\"\n",
        "        try:\n",
        "            thread = response.url\n",
        "            post_count = count_posts(html.fromstring(response.body))\n",
        "            end_page_id = math.floor(post_count / posts_per_page) * posts_per_page\n",
        "\n",
        "            for page_id in range(0, end_page_id, posts_per_page):\n",
        "                yield scrapy.Request(\n",
        "                    f\"{thread}&start={str(page_id)}\",\n",
        "                    callback=self.process_posts,\n",
        "                )\n",
        "        except IndexError:\n",
        "            return  # occurs when the requested thread doesn't exist or is empty (?)\n",
        "\n",
        "    def process_posts(self, response):\n",
        "        \"Extracts post data from a page of a thread.\"\n",
        "        thread_page_html = html.fromstring(response.body)\n",
        "        posts = get_posts(thread_page_html)\n",
        "        page_link = response.url\n",
        "        thread = page_link[page_link.find(\"&t=\") + 3 : page_link.find(\"&start\")]\n",
        "        forum = page_link[page_link.find(\"f=\") + 2 : page_link.find(\"&t=\")]\n",
        "        for post in posts:\n",
        "            yield PostItem(\n",
        "                {\"pagelink\": page_link, \"forum\": forum, \"thread\": thread, **post}\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Start scraping...\n",
        "    process = CrawlerProcess()\n",
        "    process.crawl(MafiaScumSpider)\n",
        "    process.start()\n",
        "    files.download('posts.jsonl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
